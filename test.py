import torch
import torch.nn as nn
import time
import sys
import logging
import argparse
from thop import profile
from models.vgg import Custom_Conv2d, Custom_Linear
from tqdm import tqdm
import matplotlib.pyplot as plt

from conf import settings
from utils import setup_logging, get_dataloader, count_custom_conv2d, count_custom_linear, torch_set_random_seed


def get_args():
    parser = argparse.ArgumentParser(description='train given model under given dataset')
    parser.add_argument('-net', type=str, default=None, help='the type of model to train')
    parser.add_argument('-dataset', type=str, default=None, help='the dataset to train on')
    parser.add_argument('-lr', type=float, default=settings.INITIAL_LR, help='initial learning rate')
    parser.add_argument('-lr_decay', type=float, default=settings.LR_DECAY, help='learning rate decay rate')
    parser.add_argument('-b', type=int, default=settings.BATCH_SIZE, help='batch size for dataloader')
    parser.add_argument('-warm', type=int, default=settings.WARM, help='warm up training phase')

    args = parser.parse_args()
    check_args(args)

    return args

def check_args(args: argparse.Namespace):
    if args.net is None:
        logging.error("the specific type of model should be provided, please select one of 'lenet5', 'vgg16', 'googlenet', 'resnet50', 'unet'")
        sys.exit(1)
    elif args.net not in ['lenet5', 'vgg16', 'googlenet', 'resnet50', 'unet']:
        logging.error('the specific model is not supported')
        sys.exit(1)
    if args.dataset is None:
        logging.error("the specific type of dataset to train on should be provided, please select one of 'mnist', 'cifar10', 'cifar100', 'imagenet'")
        sys.exit(1)
    elif args.dataset not in ['mnist', 'cifar10', 'cifar100', 'imagenet']:
        logging.error('the specific dataset is not supported')
        sys.exit(1)

def free_initial_weight(net):
    # free all initial weights
    for idx, layer_idx in enumerate(net.prune_choices):
        if idx <= net.last_conv_layer_idx:
            layer = net.conv_layers[layer_idx]
        else:
            layer = net.linear_layers[layer_idx]
        layer.free_original_weight()

def get_tensor_memory(tensor):
    if tensor is None:
        return 0
    return tensor.element_size() * tensor.nelement()

def test_network(target_net: nn.Module):
    correct_1 = 0.0
    correct_5 = 0.0
    loss = 0.0
    target_net.eval()
    with torch.no_grad():
        for (images, labels) in tqdm(test_loader, total=len(test_loader), desc='Testing round', unit='batch', leave=False):
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = target_net(images)
            loss += loss_function(outputs, labels).item()
            _, preds = outputs.topk(5, 1, largest=True, sorted=True)
            correct_1 += (preds[:, :1] == labels.unsqueeze(1)).sum().item()
            top5_correct = labels.view(-1, 1).expand_as(preds) == preds
            correct_5 += top5_correct.any(dim=1).sum().item()
    
    top1_acc = correct_1 / len(test_loader.dataset)
    top5_acc = correct_5 / len(test_loader.dataset)
    loss /= len(test_loader)
    FLOPs_num, para_num = profile(target_net, inputs = (input, ), verbose=False, custom_ops = custom_ops)
    return top1_acc, top5_acc, loss, FLOPs_num, para_num

def test_compression_result(original_net: nn.Module, 
                            compressed_net: nn.Module):
    top1_acc, top5_acc, loss, original_FLOPs_num, original_para_num = test_network(original_net)
    logging.info(f'Original model has loss: {loss}, top1 accuracy: {top1_acc}, top5 accuracy: {top5_acc}')
    logging.info(f'Original model has FLOPs: {original_FLOPs_num}, Parameter Num: {original_para_num}')

    top1_acc, top5_acc, loss, compressed_FLOPs_num, compressed_para_num = test_network(original_net)
    logging.info(f'Compressed model has loss: {loss}, top1 accuracy: {top1_acc}, top5 accuracy: {top5_acc}')
    logging.info(f'Compressed model has FLOPs: {compressed_FLOPs_num}, Parameter Num: {compressed_para_num}')

    FLOPs_compression_ratio = 1 - compressed_FLOPs_num / original_FLOPs_num
    Para_compression_ratio = 1 - compressed_para_num / original_para_num
    logging.info(f'FLOPS compressed ratio: {FLOPs_compression_ratio}, Parameter Num compressed ratio: {Para_compression_ratio}')
    logging.info(f'Original net: {original_net}')
    logging.info(f'Compressed net: {compressed_net}')

def show_loss(train_loss: list, 
              test_loss: list, 
              top1_acc : list):
    epoch_list = list(range(len(train_loss)))
    plt.figure()

    plt.plot(epoch_list, train_loss, label='train_loss')
    plt.plot(epoch_list, test_loss, label='test_loss')
    plt.plot(epoch_list, top1_acc, label='top1_acc')

    plt.title('Train statics')
    plt.xlabel('Epoch')
    plt.ylabel('Statics')
    plt.legend()
    plt.show()
    

if __name__ == '__main__':
    start_time = int(time.time())
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    loss_function = nn.CrossEntropyLoss()
    args = get_args()
    setup_logging(experiment_id=start_time, net=args.net, dataset=args.dataset, action='test')

    torch_set_random_seed(start_time)
    torch.manual_seed(start_time)
    logging.info(f'Start with random seed: {start_time}')

    # process input argument
    _, _, test_loader, _, _ = get_dataloader(dataset=args.dataset, batch_size=args.b)
    original_net = torch.load('models/vgg16_cifar100_Original_101.pth').to(device)
    compressed_net = torch.load('models/vgg16_cifar100_Original_101.pth').to(device)
    
    if args.net == 'lenet5':
        input = torch.rand(1, 1, 32, 32).to(device)
    else:
        input = torch.rand(1, 3, 32, 32).to(device)
    custom_ops = {Custom_Conv2d: count_custom_conv2d, Custom_Linear: count_custom_linear}
    
    # test_compression_result(original_net, compressed_net)
    train_loss = [4.285581041175082, 4.023655711537432, 3.830269047061501, 3.6584710105300866, 3.454626773629347, 3.278125073903662, 3.1202936013946143, 2.9849467503140343, 2.8657908610370764, 2.7737695091520735, 2.6927497984503237, 2.644355870878605, 2.5790162982843112, 2.5178419208282703, 2.471414162374823, 2.426753353889641, 2.3828659862508554, 2.358760020312141, 2.330586742257218, 2.2982474910023876, 2.2686519891099857, 2.251287717343596, 2.22068899001002, 2.197274851677058, 2.1914053914492087, 2.1671533349834746, 2.158114092734159, 2.1351650587433135, 2.1140778067776616, 2.121847805464664, 2.1113589194119737, 2.0881923988956927, 1.5287752825280894, 1.3763672320739082, 1.323620416929045, 1.2825788488168546, 1.2443672549694091, 1.2253730943440782, 1.2088298689373924, 1.1946972390574873, 1.1713620082801566, 1.1740333866280364, 1.1494289005503935, 1.1468323181047464, 1.1290117432089413, 1.1164576921926435, 1.112728825615495, 1.1108890337407435, 1.1019454107565039, 1.0798137963885237, 1.081996232042532, 1.0602606955696554, 1.053740155513939, 1.034908620750203, 1.032652274574465, 1.0228486035181128, 1.018311262283179, 1.0100440914978457, 0.995906677239996, 0.9869924931574965, 0.9718876665510485, 0.9684514657920583, 0.9572064625027844, 0.950840900926029, 0.9361023832769955, 0.9301866038376109, 0.9176534841127713, 0.922206505485203, 0.9022332739342204, 0.8911314027388687, 0.8847908354781168, 0.8810496770055093, 0.8780013153620083, 0.536329231794228, 0.43167589498145503, 0.39283629600196845, 0.36619134769415307, 0.3438293193764699, 0.3247335387770172, 0.30257547376177196, 0.292214842670409, 0.2744984788548611, 0.26345676278976526, 0.26254157062686617, 0.24766384636807015, 0.2366805964380579, 0.2376978207198555, 0.2259831176618176, 0.2231983140568294, 0.21670054167966404, 0.21293541280281209, 0.20935930730894092, 0.2022035662323008, 0.1893030040709259, 0.19997313873046804, 0.19649280038902828, 0.19200966371904554, 0.18895301204698775, 0.18770782689532967, 0.18433703678419522, 0.1268639243910532, 0.09877262695137497, 0.09163431542665909, 0.08213910263727235, 0.07974622489127052, 0.07916450221329699, 0.07545988781430076, 0.06966403952282865, 0.0695360720352939, 0.06928042803302674, 0.06386681285250903, 0.06750356060836245, 0.06308466842745805, 0.06064244728926045, 0.0604498411130989, 0.06027059988511722, 0.055980912640767024, 0.05558699250335584, 0.05358339276503, 0.053048901953031795, 0.052454829749548834, 0.05012935927123441, 0.04831895844825088, 0.04893264724203693, 0.04930662523593058, 0.04780820732021614, 0.047803841238303106, 0.044747600610584706, 0.04579861509277845, 0.04347773121260202, 0.04344790720659525, 0.044717230951970874, 0.04449057891902983, 0.04612984458375198, 0.04071673421639368, 0.036794735247369315, 0.03658775805645739, 0.038264541280553545, 0.035137178277468206, 0.03709635113680359, 0.036596343517922764, 0.036274708432319294, 0.033887688969702595, 0.030962731842847202, 0.03403252264475236, 0.03328073599085193, 0.03434007943791273, 0.03481265024432098, 0.033407193054790466, 0.032646957674132816, 0.0340276766292122, 0.03349329673689897, 0.03404320835652749, 0.032763387133126785, 0.03184796072652235, 0.03272426891786134, 0.032160460085505646, 0.0335774794536526, 0.03205536165729622, 0.035458165644894325, 0.03485937594481365, 0.03471757771442537, 0.03250194504044836, 0.03288581288631653, 0.0332001065120787, 0.03200829525054206, 0.03348268572684101, 0.031840740630993875, 0.0342566200015147, 0.033049686333102644, 0.03486731286158266, 0.031504849127977325, 0.033737406053620835, 0.035405961434592674, 0.03277149291761467, 0.030974953330557824, 0.032156582662116386, 0.03372423008472547, 0.03326516536831418, 0.03202734813701524, 0.032694377941186625, 0.032782304954603124, 0.034599184368253516, 0.03189268105132196, 0.03431829125465601, 0.03208050479197784, 0.03232498375563156, 0.030660904625959485, 0.03441113496766142, 0.03325130568717218, 0.032830755282288696, 0.03409484176732162, 0.03139986883958473, 0.03252076326995669, 0.03513781291782818, 0.03401858418646371, 0.032347802905832675, 0.033126000778825805, 0.03425050009627972, 0.033228877511428064]
    test_loss = [4.078727414336385, 3.9394687851773034, 3.854151372668109, 3.6784436582010005, 3.3804852057106887, 3.148735106745853, 2.976079705395276, 2.881046059765393, 3.0328327673899977, 2.6811171755006042, 2.840153863158407, 3.305792865873892, 2.5231945273242418, 2.625619028188005, 2.532996978940843, 2.4331905721109126, 2.351983003978488, 2.3196496344819852, 2.3731076958813246, 2.4477535923825036, 2.3667648502543, 2.360431991045988, 2.8670180960546565, 2.303011704094802, 2.3526457095447975, 2.171259222151358, 2.462080276465114, 2.349377890176411, 2.2702485277682922, 2.4646386315550983, 2.179321323769002, 2.2978164168852793, 1.4499160911463485, 1.4439529376693918, 1.4100818950918657, 1.3664126622525952, 1.4199747692180584, 1.3852161153962341, 1.4480140405365183, 1.4635955593253993, 1.489931362339213, 1.4621389319625082, 1.4444185691543772, 1.4319933260543436, 1.3837478251396855, 1.4865998120247563, 1.486209802235229, 1.5089041945300525, 1.437425556062143, 1.5400489070747472, 1.43360835313797, 1.4385814183874974, 1.4146771197077594, 1.495170140568214, 1.4185447798499577, 1.4399939986723889, 1.6071681191649618, 1.4451106092597865, 1.5004811694350424, 1.5519289532794227, 1.4030955584743354, 1.4353601540191263, 1.5279705856419816, 1.478626540944546, 1.507950009424475, 1.4976934036122094, 1.4707868597175502, 1.469955429246154, 1.4674026762382895, 1.5039613684521447, 1.5277870154079003, 1.4380395125739183, 1.5020091895815693, 1.2006200657615178, 1.2248338717448561, 1.2325257764586919, 1.2619705724565289, 1.276409871970551, 1.2906904982615122, 1.3355568360678758, 1.3331795742240133, 1.3646418927591057, 1.379512170447579, 1.3913188961487781, 1.4229559264605558, 1.4367431002327158, 1.425625343865986, 1.4575110575820827, 1.4376173958748202, 1.4574766226961642, 1.4781118691722048, 1.5030448376377927, 1.5050153377689892, 1.529294400275508, 1.5215331466892097, 1.5265262164647067, 1.5499787647512895, 1.4961259123645252, 1.544277670262735, 1.5789418446866772, 1.46267886176894, 1.4746867901162257, 1.4721520942977713, 1.5088729239717316, 1.5073428169081482, 1.523111866244787, 1.5435976235172417, 1.5588608987723724, 1.5490010803258871, 1.5572055517872678, 1.5682177241844466, 1.5900485741941235, 1.567472206640847, 1.5873538663115683, 1.5829596557194674, 1.5899022644079184, 1.5804266310945343, 1.6245465338984622, 1.6178917198241511, 1.6249518998061554, 1.6227188276339182, 1.6292091208168222, 1.6546597224247606, 1.6339718616461452, 1.6495274999473668, 1.633322532418408, 1.642755085908914, 1.6469357926634294, 1.6486810724946517, 1.6343181412431258, 1.644030451020108, 1.6248624713360509, 1.6524654468403588, 1.6556532933742185, 1.6490235834182063, 1.6383550702771055, 1.6382015468199043, 1.6420670657218257, 1.6410819774941554, 1.6385745655132244, 1.6336760686922678, 1.654450410529028, 1.6242847636907916, 1.641111533098583, 1.6329879006253014, 1.6294012326228469, 1.6317001430294182, 1.6308871130400067, 1.6633058062082604, 1.6373425375057171, 1.6362210693238657, 1.6177141741861272, 1.632465496847901, 1.6325757744946057, 1.6280201145365267, 1.6532261613049084, 1.6403180858756923, 1.642530382434024, 1.625912762140926, 1.633281184902674, 1.6280643607996688, 1.6307584477376333, 1.6290934334827374, 1.6259809019444864, 1.6476992398877688, 1.6260085935834088, 1.6366818796230267, 1.6474918013886561, 1.6421159287042255, 1.6365162737761871, 1.636899492408656, 1.622837494068508, 1.6390871081171157, 1.6319388800029513, 1.6368239982218682, 1.6345671098443526, 1.6466330093673514, 1.6498081525669823, 1.6452375574956966, 1.6397198747984971, 1.617359799179661, 1.6354401217231267, 1.641975134233885, 1.6340191228480279, 1.6422943706753887, 1.6420782932752296, 1.632548996919318, 1.6328959842271442, 1.6348535011086283, 1.6420696083503434, 1.643653514264505, 1.6458367916602124, 1.6594873696942873, 1.6213462352752686, 1.627019936525369, 1.6323895771292192, 1.6408172634583484, 1.634328855743891, 1.631822779208799, 1.6450894437258756]
    top1_acc = [0.0482, 0.0713, 0.0915, 0.1179, 0.1682, 0.2133, 0.2313, 0.2476, 0.2316, 0.2996, 0.2829, 0.2517, 0.3433, 0.3372, 0.3524, 0.3687, 0.3905, 0.3935, 0.3814, 0.3803, 0.3861, 0.3967, 0.3398, 0.4125, 0.416, 0.4359, 0.3865, 0.4105, 0.4259, 0.3996, 0.4446, 0.4318, 0.6011, 0.6006, 0.6107, 0.6249, 0.6168, 0.6157, 0.6056, 0.6086, 0.6119, 0.6102, 0.6152, 0.6149, 0.6313, 0.6144, 0.6121, 0.6078, 0.6201, 0.6021, 0.6237, 0.6178, 0.6305, 0.6176, 0.6288, 0.628, 0.5904, 0.6258, 0.6154, 0.6088, 0.6314, 0.6333, 0.6117, 0.6253, 0.6296, 0.6222, 0.6306, 0.62, 0.6244, 0.6172, 0.6178, 0.6347, 0.6281, 0.7022, 0.7002, 0.7065, 0.7006, 0.7024, 0.7053, 0.7011, 0.7033, 0.7042, 0.6983, 0.6997, 0.6954, 0.6955, 0.6998, 0.6982, 0.6956, 0.6937, 0.6954, 0.6909, 0.6941, 0.6955, 0.6937, 0.6932, 0.6867, 0.6971, 0.6921, 0.6872, 0.7097, 0.7121, 0.7126, 0.7101, 0.7126, 0.7119, 0.7126, 0.7107, 0.7138, 0.7145, 0.7121, 0.7112, 0.7131, 0.7128, 0.715, 0.7143, 0.7124, 0.7112, 0.7113, 0.7129, 0.7105, 0.7117, 0.7157, 0.7138, 0.7131, 0.7147, 0.7125, 0.7124, 0.7127, 0.7099, 0.7142, 0.7154, 0.715, 0.716, 0.7173, 0.7142, 0.716, 0.7153, 0.7157, 0.7171, 0.7179, 0.7163, 0.7168, 0.7186, 0.7158, 0.7177, 0.7168, 0.7162, 0.7158, 0.7168, 0.7171, 0.7173, 0.7166, 0.717, 0.7158, 0.7155, 0.7166, 0.7159, 0.716, 0.7171, 0.7171, 0.7155, 0.716, 0.7156, 0.7148, 0.7163, 0.7167, 0.7153, 0.7157, 0.7178, 0.7156, 0.718, 0.7174, 0.7139, 0.7163, 0.7161, 0.7158, 0.7166, 0.7164, 0.7157, 0.7148, 0.7162, 0.7159, 0.7173, 0.7159, 0.7178, 0.7157, 0.716, 0.7167, 0.7158, 0.7159, 0.716, 0.7181, 0.7165, 0.7179, 0.7164, 0.7158, 0.7158, 0.716, 0.7171]
    show_loss(train_loss, test_loss, top1_acc)
    